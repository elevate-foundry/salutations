# Model Configuration
MODEL_A_PATH=meta-llama/Llama-3.1-8B
MODEL_B_PATH=mistralai/Mistral-7B-v0.1
MODEL_C_PATH=microsoft/Phi-3-mini-4k-instruct

# Model A specialization (reasoning, general)
MODEL_A_ROLE=reasoning
# Model B specialization (search, facts)
MODEL_B_ROLE=search
# Model C specialization (code, technical)
MODEL_C_ROLE=code

# Quantization (4bit, 8bit, or none)
MODEL_QUANTIZATION=8bit

# Device configuration
DEVICE=cuda
DEVICE_MAP=auto

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password_here
NEO4J_DATABASE=neo4j

# MCP Server Configuration
MCP_SERVER_HOST=localhost
MCP_SERVER_PORT=3000
PLAYWRIGHT_HEADLESS=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# Braiding Configuration
FUSION_STRATEGY=learned_weighted  # learned_weighted, attention, router
FUSION_CHECKPOINT_PATH=./checkpoints/fusion_layer.pt
ENABLE_LAYER_CACHE=true

# Search Configuration
SEARCH_ENGINE=duckduckgo  # google, bing, duckduckgo
MAX_SEARCH_RESULTS=10
SEARCH_TIMEOUT=30

# Memory Configuration
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
VECTOR_DIM=384
MAX_MEMORY_CONTEXT=10

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/salutations.log
